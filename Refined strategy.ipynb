{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d866b2a5",
   "metadata": {},
   "source": [
    "# Imports & Seeds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "279369b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 0. Imports & Seeds ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import (Input, Conv1D, MaxPooling1D, LSTM, Dense, Dropout,\n",
    "                                     BatchNormalization, Layer)\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1775c1ba",
   "metadata": {},
   "source": [
    "# Custom Layers & Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7525af71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 1. Custom Layers & Loss ===\n",
    "class F1Score(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name='f1_score', **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.precision = Precision()\n",
    "        self.recall = Recall()\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        self.precision.update_state(y_true, y_pred, sample_weight)\n",
    "        self.recall.update_state(y_true, y_pred, sample_weight)\n",
    "    def result(self):\n",
    "        p = self.precision.result()\n",
    "        r = self.recall.result()\n",
    "        return 2 * ((p * r) / (p + r + K.epsilon()))\n",
    "    def reset_state(self):\n",
    "        self.precision.reset_state()\n",
    "        self.recall.reset_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09fe350c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "    def call(self, y_true, y_pred):\n",
    "        y_pred = K.clip(y_pred, K.epsilon(), 1.0 - K.epsilon())\n",
    "        ce = -y_true * K.log(y_pred)\n",
    "        loss = self.alpha * K.pow(1.0 - y_pred, self.gamma) * ce\n",
    "        return K.mean(loss, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f002e891",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name='att_weight',\n",
    "                                 shape=(input_shape[-1], input_shape[-1]),\n",
    "                                 initializer='glorot_uniform',\n",
    "                                 trainable=True)\n",
    "        self.b = self.add_weight(name='att_bias',\n",
    "                                 shape=(input_shape[-1],),\n",
    "                                 initializer='zeros',\n",
    "                                 trainable=True)\n",
    "        self.u = self.add_weight(name='att_u',\n",
    "                                 shape=(input_shape[-1], 1),  # âœ… 2D shape!\n",
    "                                 initializer='glorot_uniform',\n",
    "                                 trainable=True)\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        uit = K.tanh(K.bias_add(K.dot(x, self.W), self.b))  # shape: (batch, timesteps, features)\n",
    "        ait = K.dot(uit, self.u)                            # shape: (batch, timesteps, 1)\n",
    "        ait = K.squeeze(ait, -1)                            # shape: (batch, timesteps)\n",
    "        a = K.softmax(ait)                                  # shape: (batch, timesteps)\n",
    "        a = K.expand_dims(a, -1)                            # shape: (batch, timesteps, 1)\n",
    "        return K.sum(x * a, axis=1)                         # shape: (batch, features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767f35ae",
   "metadata": {},
   "source": [
    "# Load & Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7cc18ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "can_data = pd.read_csv(r\"G:\\road\\signal_extractions\\attacks\\correlated_signal_attack_1_masquerade.csv\")\n",
    "can_data.fillna(0, inplace=True)\n",
    "\n",
    "signal_columns = [col for col in can_data.columns if \"Signal\" in col]\n",
    "can_data['Time'] = pd.to_datetime(can_data['Time'], unit='s')\n",
    "can_data = can_data.sort_values('Time').set_index('Time')\n",
    "can_data[signal_columns] = can_data[signal_columns].apply(pd.to_numeric, errors='coerce')\n",
    "can_data = can_data.resample('100us').mean().interpolate(method='cubic').reset_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cb72b2",
   "metadata": {},
   "source": [
    "#  Sliding Window Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ac92f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_time_series(data, window_size=10, step_size=1):\n",
    "    sequences, labels = [], []\n",
    "    for i in range(0, len(data) - window_size, step_size):\n",
    "        window = data.iloc[i:i + window_size]\n",
    "        sequences.append(window[signal_columns].values)\n",
    "        label = 1 if window['Label'].sum() > 0 else 0  # Majority or any attack\n",
    "        labels.append(label)\n",
    "    return np.array(sequences), np.array(labels)\n",
    "\n",
    "X, y = create_time_series(can_data)\n",
    "X = np.nan_to_num(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8b43b1",
   "metadata": {},
   "source": [
    "# Scaling & Balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7f7bbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)\n",
    "X_test = scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eac1b5c",
   "metadata": {},
   "source": [
    "Oversampling minority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "706bebc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(X_train.reshape(X_train.shape[0], -1))\n",
    "train_df['label'] = y_train\n",
    "maj = train_df[train_df['label'] == 0]\n",
    "min_ = train_df[train_df['label'] == 1]\n",
    "min_upsampled = resample(min_, replace=True, n_samples=len(maj), random_state=42)\n",
    "balanced = pd.concat([maj, min_upsampled])\n",
    "X_train = balanced.drop('label', axis=1).values.reshape(-1, X_train.shape[1], X_train.shape[2])\n",
    "y_train = balanced['label'].values\n",
    "\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9804533a",
   "metadata": {},
   "source": [
    "# Model Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7dbcd0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_lstm_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Conv1D(64, 3, activation='relu', padding='same', input_shape=input_shape),\n",
    "        MaxPooling1D(2),\n",
    "        Dropout(0.2),\n",
    "        LSTM(64, return_sequences=True),\n",
    "        Dropout(0.3),\n",
    "        LSTM(32),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f434e298",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_attention_model(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = Conv1D(64, 3, activation='relu', padding='same')(inputs)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "    x = LSTM(64, return_sequences=True)(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = LSTM(32, return_sequences=True)(x)\n",
    "    x = AttentionLayer()(x)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    output = Dense(1, activation='sigmoid')(x)\n",
    "    return Model(inputs, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b93ca6",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "247a54ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 64\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "af27a5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training CNN_LSTM ---\n",
      "Epoch 1/20\n",
      "5162/5162 - 63s - loss: 0.6921 - accuracy: 0.5097 - precision_6: 0.5112 - recall_6: 0.4429 - f1_score: 0.4746 - val_loss: 0.6940 - val_accuracy: 0.3763 - val_precision_6: 0.3763 - val_recall_6: 1.0000 - val_f1_score: 0.5468 - 63s/epoch - 12ms/step\n",
      "Epoch 2/20\n",
      "5162/5162 - 62s - loss: 0.6927 - accuracy: 0.5075 - precision_6: 0.5074 - recall_6: 0.5104 - f1_score: 0.5089 - val_loss: 0.6871 - val_accuracy: 0.6238 - val_precision_6: 1.0000 - val_recall_6: 8.0292e-05 - val_f1_score: 1.6057e-04 - 62s/epoch - 12ms/step\n",
      "Epoch 3/20\n",
      "5162/5162 - 61s - loss: 0.6880 - accuracy: 0.5275 - precision_6: 0.5399 - recall_6: 0.3719 - f1_score: 0.4404 - val_loss: 0.6911 - val_accuracy: 0.6253 - val_precision_6: 0.7550 - val_recall_6: 0.0061 - val_f1_score: 0.0120 - 61s/epoch - 12ms/step\n",
      "Epoch 4/20\n",
      "5162/5162 - 72s - loss: 0.6806 - accuracy: 0.5469 - precision_6: 0.5756 - recall_6: 0.3567 - f1_score: 0.4405 - val_loss: 0.6929 - val_accuracy: 0.6239 - val_precision_6: 0.6170 - val_recall_6: 0.0012 - val_f1_score: 0.0023 - 72s/epoch - 14ms/step\n",
      "Epoch 5/20\n",
      "5162/5162 - 78s - loss: 0.6649 - accuracy: 0.5811 - precision_6: 0.6488 - recall_6: 0.3534 - f1_score: 0.4576 - val_loss: 0.6668 - val_accuracy: 0.6763 - val_precision_6: 0.6560 - val_recall_6: 0.2936 - val_f1_score: 0.4056 - 78s/epoch - 15ms/step\n",
      "Epoch 6/20\n",
      "5162/5162 - 78s - loss: 0.6546 - accuracy: 0.5985 - precision_6: 0.6807 - recall_6: 0.3710 - f1_score: 0.4802 - val_loss: 0.6637 - val_accuracy: 0.6248 - val_precision_6: 0.7874 - val_recall_6: 0.0040 - val_f1_score: 0.0080 - 78s/epoch - 15ms/step\n",
      "Epoch 7/20\n",
      "5162/5162 - 75s - loss: 0.6491 - accuracy: 0.6076 - precision_6: 0.6955 - recall_6: 0.3829 - f1_score: 0.4939 - val_loss: 0.6612 - val_accuracy: 0.6252 - val_precision_6: 0.7487 - val_recall_6: 0.0060 - val_f1_score: 0.0119 - 75s/epoch - 15ms/step\n",
      "Epoch 8/20\n",
      "5162/5162 - 77s - loss: 0.6446 - accuracy: 0.6147 - precision_6: 0.7090 - recall_6: 0.3891 - f1_score: 0.5024 - val_loss: 0.6529 - val_accuracy: 0.6365 - val_precision_6: 0.9335 - val_recall_6: 0.0366 - val_f1_score: 0.0705 - 77s/epoch - 15ms/step\n",
      "Epoch 9/20\n",
      "5162/5162 - 74s - loss: 0.6448 - accuracy: 0.6144 - precision_6: 0.6997 - recall_6: 0.4009 - f1_score: 0.5098 - val_loss: 0.6319 - val_accuracy: 0.6604 - val_precision_6: 0.9459 - val_recall_6: 0.1033 - val_f1_score: 0.1862 - 74s/epoch - 14ms/step\n",
      "Epoch 10/20\n",
      "5162/5162 - 76s - loss: 0.6437 - accuracy: 0.6159 - precision_6: 0.7091 - recall_6: 0.3930 - f1_score: 0.5057 - val_loss: 0.6405 - val_accuracy: 0.6553 - val_precision_6: 0.9599 - val_recall_6: 0.0876 - val_f1_score: 0.1605 - 76s/epoch - 15ms/step\n",
      "Epoch 11/20\n",
      "5162/5162 - 75s - loss: 0.6417 - accuracy: 0.6206 - precision_6: 0.7141 - recall_6: 0.4022 - f1_score: 0.5146 - val_loss: 0.6335 - val_accuracy: 0.6639 - val_precision_6: 0.9436 - val_recall_6: 0.1135 - val_f1_score: 0.2026 - 75s/epoch - 15ms/step\n",
      "Epoch 12/20\n",
      "5162/5162 - 76s - loss: 0.6366 - accuracy: 0.6276 - precision_6: 0.7274 - recall_6: 0.4081 - f1_score: 0.5228 - val_loss: 0.6362 - val_accuracy: 0.6616 - val_precision_6: 0.9559 - val_recall_6: 0.1053 - val_f1_score: 0.1898 - 76s/epoch - 15ms/step\n",
      "Epoch 13/20\n",
      "5162/5162 - 80s - loss: 0.6359 - accuracy: 0.6283 - precision_6: 0.7319 - recall_6: 0.4051 - f1_score: 0.5215 - val_loss: 0.6354 - val_accuracy: 0.6639 - val_precision_6: 0.9418 - val_recall_6: 0.1138 - val_f1_score: 0.2030 - 80s/epoch - 16ms/step\n",
      "Epoch 14/20\n",
      "5162/5162 - 78s - loss: 0.6341 - accuracy: 0.6312 - precision_6: 0.7397 - recall_6: 0.4048 - f1_score: 0.5232 - val_loss: 0.6378 - val_accuracy: 0.6603 - val_precision_6: 0.9546 - val_recall_6: 0.1021 - val_f1_score: 0.1844 - 78s/epoch - 15ms/step\n",
      "\n",
      "--- Training ATTENTION ---\n",
      "Epoch 1/20\n",
      "5162/5162 - 89s - loss: 0.6932 - accuracy: 0.5005 - precision_8: 0.5006 - recall_8: 0.4240 - f1_score: 0.4591 - val_loss: 0.6919 - val_accuracy: 0.6237 - val_precision_8: 0.0000e+00 - val_recall_8: 0.0000e+00 - val_f1_score: 0.0000e+00 - 89s/epoch - 17ms/step\n",
      "Epoch 2/20\n",
      "5162/5162 - 78s - loss: 0.6932 - accuracy: 0.5014 - precision_8: 0.5014 - recall_8: 0.5056 - f1_score: 0.5035 - val_loss: 0.6949 - val_accuracy: 0.3763 - val_precision_8: 0.3763 - val_recall_8: 1.0000 - val_f1_score: 0.5468 - 78s/epoch - 15ms/step\n",
      "Epoch 3/20\n",
      "5162/5162 - 79s - loss: 0.6932 - accuracy: 0.5006 - precision_8: 0.5006 - recall_8: 0.4513 - f1_score: 0.4747 - val_loss: 0.6952 - val_accuracy: 0.3763 - val_precision_8: 0.3763 - val_recall_8: 1.0000 - val_f1_score: 0.5468 - 79s/epoch - 15ms/step\n",
      "Epoch 4/20\n",
      "5162/5162 - 80s - loss: 0.6932 - accuracy: 0.4987 - precision_8: 0.4988 - recall_8: 0.5267 - f1_score: 0.5123 - val_loss: 0.6921 - val_accuracy: 0.6237 - val_precision_8: 0.0000e+00 - val_recall_8: 0.0000e+00 - val_f1_score: 0.0000e+00 - 80s/epoch - 16ms/step\n",
      "Epoch 5/20\n",
      "5162/5162 - 79s - loss: 0.6932 - accuracy: 0.5000 - precision_8: 0.5000 - recall_8: 0.4368 - f1_score: 0.4663 - val_loss: 0.6973 - val_accuracy: 0.3763 - val_precision_8: 0.3763 - val_recall_8: 1.0000 - val_f1_score: 0.5468 - 79s/epoch - 15ms/step\n",
      "Epoch 6/20\n",
      "5162/5162 - 81s - loss: 0.6932 - accuracy: 0.4991 - precision_8: 0.4992 - recall_8: 0.5706 - f1_score: 0.5325 - val_loss: 0.6932 - val_accuracy: 0.3763 - val_precision_8: 0.3763 - val_recall_8: 1.0000 - val_f1_score: 0.5468 - 81s/epoch - 16ms/step\n"
     ]
    }
   ],
   "source": [
    "for name, model_func in {\n",
    "    'cnn_lstm': create_cnn_lstm_model,\n",
    "    'attention': create_attention_model\n",
    "}.items():\n",
    "    print(f\"\\n--- Training {name.upper()} ---\")\n",
    "    model = model_func(input_shape)\n",
    "    model.compile(optimizer=Adam(0.001, clipnorm=1.0), loss='binary_crossentropy',\n",
    "                  metrics=['accuracy', Precision(), Recall(), F1Score()])\n",
    "    model.fit(X_train, y_train, validation_data=(X_test, y_test),\n",
    "              epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=[early_stop], verbose=2)\n",
    "    models[name] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "87533a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label Distribution in Training Set:\n",
      "label\n",
      "0    0.5\n",
      "1    0.5\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# print label distributions\n",
    "print(\"\\nLabel Distribution in Training Set:\")\n",
    "print(balanced['label'].value_counts(normalize=True))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v2x_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
